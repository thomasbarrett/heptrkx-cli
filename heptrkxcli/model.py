"""
adapted from deepmind/graph_nets/demos/models.py
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
from graph_nets import modules
from graph_nets import utils_tf
import sonnet as snt

NUM_LAYERS = 4    # Hard-code number of layers in the edge/node/global models.
LATENT_SIZE = 64  # Hard-code latent layer sizes for demos.

def make_mlp_model():
    """Instantiates a new MLP, followed by LayerNorm.
    The parameters of each new MLP are not shared with others generated by
    this function.
    Returns:
    A Sonnet module which contains the MLP and LayerNorm.
    """
    return snt.Sequential([
        snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
        snt.LayerNorm()
    ])

class MLPGraphIndependent(snt.AbstractModule):
    """GraphIndependent with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphIndependent"):
        super(MLPGraphIndependent, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphIndependent(
                edge_model_fn=make_mlp_model,
                node_model_fn=make_mlp_model,
                global_model_fn=None)

    def _build(self, inputs):
        return self._network(inputs)



class SegmentClassifier(snt.AbstractModule):

    def __init__(self, name="SegmentClassifier"):
        """ The SegmentClassifier module performs binary edge classification using
        an encoder-decoder graph neural network model. 

        The 'encoder' and 'decoder' fields are derived from GraphIndependent
        modules from the deepmind/graph_nets library. These perform an operation
        on the nodes and edges indepedently and in parallel. The 'core' field
        is an InteractionNetwork from the deepmind/graph_nets library which
        performs message passing between nodes and edges.
        """

        super(SegmentClassifier, self).__init__(name=name)

        # Set the module to be used for the encoding step.
        self._encoder = MLPGraphIndependent()

        # Set the module to be used for the core step
        self._core = modules.InteractionNetwork(
            edge_model_fn=make_mlp_model,
            node_model_fn=make_mlp_model,
            reducer=tf.unsorted_segment_sum
        )

        # Set the module to be used for the decoding step
        self._decoder = MLPGraphIndependent()

        # Transforms the outputs into appropriate shapes.
        edge_classification_fn = lambda: snt.Sequential([
            snt.nets.MLP(
                [LATENT_SIZE/2, 1],
                activation=tf.nn.relu, # default activation function
                name='edge_output'
            ),
            tf.sigmoid
        ])

        # This line is needed in order for the 'edge_fn' tensorflow variables to 
        # be properly placed in the SegmentClassifier scope. A full explanation
        # can be found in the sonnet documentation at the url (https://sonnet.dev/)
        with self._enter_variable_scope():
            self._output_transform = modules.GraphIndependent(edge_classification_fn, None, None)

    def _build(self, input_op, num_processing_steps):
        """
        This method is called every time that a new SegmentClassifier module
        needs to be constructed and inserted into a tensorflow computation graph.
        A full explanation can be found in the sonnet documentation at the url
        (https://sonnet.dev/).
        """
        
        # The encoding step transforms input feaures into some learned internal
        # representation. The `latent0` variable holds the transformed graph
        # The `latent` variable is initialized to latent variable but will
        # be changed during each iteration of the core step.
        latent0 = self._encoder(input_op)
        latent = latent0

        # Core: Iteratively processes internal graph representation.
        # Adjusting num_processing_sets decreases the size of the core and 
        # reduces computational complexity of the GNN. Note that the input
        # to the nth interation of self._core is the output of the (n-1)th
        # iteration of self._core concatenated along the feature axis with
        # the input to the 0th iteration. In short, the features of each node
        # and edge are the original transformed features as well as the output
        # features from the previous iteration.
        #
        # I believe that the input to each core layer is structured this
        # way so that the model does not 'forget' original constant graph
        # features such as position and distance deep inside the
        # computational graph. Although they should propogate through the
        # network to some extend, directly passing along the original 
        # values provides simple numerical stability.
        output_ops = []

        for _ in range(num_processing_steps):
            core_input = utils_tf.concat([latent0, latent], axis=1)
            latent = self._core(core_input)
            decoded_op = self._decoder(latent)
            output_ops.append(self._output_transform(decoded_op))

        return output_ops